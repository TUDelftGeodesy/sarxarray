{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SARXarray","text":"<p>SARXarray is an open-source Xarray extension for Synthetic Aperture Radar (SAR) data.</p> <p>SARXarray is especially designed to work with complex data, that is, containing both the phase and amplitude of the data. The extension can handle coregistered stacks of Single Look Complex (SLC) data, as well as derived products such as interferogram stacks. It utilizes Xarray\u2019s support on labeled multi-dimensional datasets to stress the space-time character of the image stacks. Dask Array is implemented to support parallel computation.</p> <p>SARXarry supports the following functionalities:</p> <ol> <li> <p>Chunk-wise reading/writing of coregistered SLC or interferogram stacks;</p> </li> <li> <p>Basic operations on complex data, e.g., averaging along axis and complex conjugate multiplication;</p> </li> <li> <p>Specific SAR data operations, e.g., multi-looking and coherence estimation.</p> </li> </ol> <p>All the above functionalities can be scaled up to a Hyper-Performance Computation (HPC) system.</p>"},{"location":"CHANGELOG/","title":"Change Log","text":"<p>All notable changes to this project will be documented in this file. This project adheres to Semantic Versioning.</p> <p>See releases in the sarxarray repository for more details.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>This code of conduct is adapted from the  Git Code of Conduct.</p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at team-atlas@esciencecenter.nl.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All Project maintainers are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4,  available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"SARXarray Contributing Guidelines","text":"<p>We welcome any kind of contribution to our software, from a simple comment  or question to a full fledged pull request.  Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ul> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation).</li> </ul> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you-have-a-question","title":"You have a question","text":"<ul> <li>use the search functionality in GitHub issue to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, create a new issue;</li> <li>add the \"question\" label; include other labels when relevant.</li> </ul>"},{"location":"CONTRIBUTING/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ul> <li>use the search functionality in GitHub issue to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, create a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>add relevant labels to the newly created issue.</li> </ul>"},{"location":"CONTRIBUTING/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ul> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions from GitHub: instruction 1: configuring a remote for a fork and instruction 2: syncing a fork);</li> <li>install the pre-commit hooks by running <code>pre-commit install</code> in the project root directory;</li> <li>make sure the existing tests still work by running, e.g. <code>pytest tests</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>make sure the linting tests pass by running <code>ruff</code> in the project root directory: <code>ruff check .</code>;</li> <li>push your feature branch to (your fork of) the sarxarray repository on GitHub;</li> <li>create the pull request, e.g. following the instructions: creating a pull request.</li> </ul> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"api_reference/","title":"API reference","text":"<p>Here is the API reference for the <code>sarxarray</code> package.</p>"},{"location":"api_reference/#slc-stack-module","title":"SLC stack module","text":""},{"location":"api_reference/#sarxarray.stack.Stack","title":"sarxarray.stack.Stack","text":"<pre><code>Stack(xarray_obj)\n</code></pre> <p>Methods:</p> <ul> <li> <code>mrm</code>             \u2013              <p>Compute a Mean Reflection Map (MRM).</p> </li> <li> <code>multi_look</code>             \u2013              <p>Perform multi-looking on a Stack, and return a Stack.</p> </li> <li> <code>point_selection</code>             \u2013              <p>Select pixels from a Stack, and return a Space-Time Matrix.</p> </li> </ul> Source code in <code>sarxarray/stack.py</code> <pre><code>def __init__(self, xarray_obj):\n    self._obj = xarray_obj\n</code></pre>"},{"location":"api_reference/#sarxarray.stack.Stack.mrm","title":"mrm","text":"<pre><code>mrm()\n</code></pre> <p>Compute a Mean Reflection Map (MRM).</p> Source code in <code>sarxarray/stack.py</code> <pre><code>def mrm(self):\n    \"\"\"Compute a Mean Reflection Map (MRM).\"\"\"\n    t_order = list(self._obj.sizes).index(\"time\")  # Time dimension index\n    return self._obj.amplitude.mean(axis=t_order)\n</code></pre>"},{"location":"api_reference/#sarxarray.stack.Stack.multi_look","title":"multi_look","text":"<pre><code>multi_look(window_size, method='coarsen', statistics='mean', compute=True)\n</code></pre> <p>Perform multi-looking on a Stack, and return a Stack.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>The data to be multi-looked.</p> </li> <li> <code>window_size</code>               (<code>tuple</code>)           \u2013            <p>Window size for multi-looking, in the format of (azimuth, range)</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'coarsen'</code> )           \u2013            <p>Method of multi-looking, by default \"coarsen\"</p> </li> <li> <code>statistics</code>               (<code>str</code>, default:                   <code>'mean'</code> )           \u2013            <p>Statistics method for multi-looking, by default \"mean\"</p> </li> <li> <code>compute</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the result, by default True. If False, the result will be <code>dask.delayed.Delayed</code>. This is useful when the multi_look is used as an intermediate result.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>An <code>xarray.Dataset</code> with coarsen shape if <code>compute</code> is True, otherwise a <code>dask.delayed.Delayed</code> object.</p> </li> </ul> Source code in <code>sarxarray/stack.py</code> <pre><code>def multi_look(\n    self, window_size, method=\"coarsen\", statistics=\"mean\", compute=True\n):\n    \"\"\"Perform multi-looking on a Stack, and return a Stack.\n\n    Parameters\n    ----------\n    data : xarray.Dataset\n        The data to be multi-looked.\n    window_size : tuple\n        Window size for multi-looking, in the format of (azimuth, range)\n    method : str, optional\n        Method of multi-looking, by default \"coarsen\"\n    statistics : str, optional\n        Statistics method for multi-looking, by default \"mean\"\n    compute : bool, optional\n        Whether to compute the result, by default True. If False, the result\n        will be `dask.delayed.Delayed`. This is useful when the multi_look\n        is used as an intermediate result.\n\n    Returns\n    -------\n    xarray.Dataset\n        An `xarray.Dataset` with coarsen shape if\n        `compute` is True, otherwise a `dask.delayed.Delayed` object.\n    \"\"\"\n    return multi_look(self._obj, window_size, method, statistics, compute)\n</code></pre>"},{"location":"api_reference/#sarxarray.stack.Stack.point_selection","title":"point_selection","text":"<pre><code>point_selection(threshold, method='amplitude_dispersion', chunks=1000)\n</code></pre> <p>Select pixels from a Stack, and return a Space-Time Matrix.</p> <p>The selection method is defined by <code>method</code> and <code>threshold</code>. The selected pixels will be reshaped to (space, time), where <code>space</code> is the number of selected pixels. The unselected pixels will be discarded. The original <code>azimuth</code> and <code>range</code> coordinates will be persisted.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Threshold value for selection</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'amplitude_dispersion'</code> )           \u2013            <p>Method of selection, by default \"amplitude_dispersion\"</p> </li> <li> <code>chunks</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Chunk size in the space dimension, by default 1000</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>An xarray.Dataset with two dimensions: (space, time).</p> </li> </ul> Source code in <code>sarxarray/stack.py</code> <pre><code>def point_selection(self, threshold, method=\"amplitude_dispersion\", chunks=1000):\n    \"\"\"Select pixels from a Stack, and return a Space-Time Matrix.\n\n    The selection method is defined by `method` and `threshold`.\n    The selected pixels will be reshaped to (space, time), where `space` is\n    the number of selected pixels. The unselected pixels will be discarded.\n    The original `azimuth` and `range` coordinates will be persisted.\n\n    Parameters\n    ----------\n    threshold : float\n        Threshold value for selection\n    method : str, optional\n        Method of selection, by default \"amplitude_dispersion\"\n    chunks : int, optional\n        Chunk size in the space dimension, by default 1000\n\n    Returns\n    -------\n    xarray.Dataset\n        An xarray.Dataset with two dimensions: (space, time).\n    \"\"\"\n    match method:\n        case \"amplitude_dispersion\":\n            # Amplitude dispersion thresholding\n            # Note there can be NaN values in the amplitude dispersion\n            # However NaN values will not pass this threshold\n            mask = self._amp_disp() &lt; threshold\n        case _:\n            raise NotImplementedError\n\n    # Get the 1D index on space dimension\n    mask_1d = mask.stack(space=(\"azimuth\", \"range\")).drop_vars(\n        [\"azimuth\", \"range\", \"space\"]\n    )\n    index = mask_1d.space.data[mask_1d.data]  # Evaluate the mask\n\n    # Reshape from Stack (\"azimuth\", \"range\", \"time\") to Space-Time Matrix\n    # (\"space\", \"time\")\n    stacked = self._obj.stack(space=(\"azimuth\", \"range\"))\n    stm = stacked.drop_vars(\n        [\"space\", \"azimuth\", \"range\"]\n    )  # this will also drop azimuth and range\n    stm = stm.assign_coords(\n        {\n            \"azimuth\": ([\"space\"], stacked.azimuth.data),\n            \"range\": ([\"space\"], stacked.range.data),\n        }\n    )  # keep azimuth and range index\n\n    # Apply selection\n    stm_masked = stm.sel(space=index)\n\n    # Re-order the dimensions to\n    # community preferred (\"space\", \"time\") order\n    # Since there are dask arrays in stm_masked,\n    # this operation is lazy.\n    # Therefore its effect can be observed after evaluation\n    stm_masked = stm_masked.transpose(\"space\", \"time\")\n\n    # Rechunk is needed because after apply maksing,\n    # the chunksize will be in consistant\n    stm_masked = stm_masked.chunk(\n        {\n            \"space\": chunks,\n            \"time\": -1,\n        }\n    )\n\n    return stm_masked\n</code></pre>"},{"location":"api_reference/#io-module","title":"I/O module","text":""},{"location":"api_reference/#sarxarray._io.from_dataset","title":"sarxarray._io.from_dataset","text":"<pre><code>from_dataset(ds: Dataset) -&gt; Dataset\n</code></pre> <p>Create a SLC stack or from an Xarray Dataset.</p> <p>This function create tasks graph converting the two data variables of complex data: <code>real</code> and <code>imag</code>, to three variables: <code>complex</code>, <code>amplitude</code>, and <code>phase</code>.</p> <p>The function is intented for an SLC stack in <code>xr.Dataset</code> loaded from a Zarr file.</p> <p>For other datasets, such as lat, lon, etc., please use <code>xr.open_zarr</code> directly.</p> <p>Parameters:</p> <ul> <li> <code>ds</code>               (<code>Dataset</code>)           \u2013            <p>SLC stack loaded from a Zarr file. Must have three dimensions: <code>(azimuth, range, time)</code>. Must have two variables: <code>real</code> and <code>imag</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>Converted SLC stack. An xarray.Dataset with three dimensions: <code>(azimuth, range, time)</code>, and three variables: <code>complex</code>, <code>amplitude</code>, <code>phase</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>The input dataset should have three dimensions: <code>(azimuth, range, time)</code>.</p> </li> <li> <code>ValueError</code>             \u2013            <p>The input dataset should have the following variables: <code>('real', 'imag')</code>.</p> </li> </ul> Source code in <code>sarxarray/_io.py</code> <pre><code>def from_dataset(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Create a SLC stack or from an Xarray Dataset.\n\n    This function create tasks graph converting the two data variables of complex data:\n    `real` and `imag`, to three variables: `complex`, `amplitude`, and `phase`.\n\n    The function is intented for an SLC stack in `xr.Dataset` loaded from a Zarr file.\n\n    For other datasets, such as lat, lon, etc., please use `xr.open_zarr` directly.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        SLC stack loaded from a Zarr file.\n        Must have three dimensions: `(azimuth, range, time)`.\n        Must have two variables: `real` and `imag`.\n\n    Returns\n    -------\n    xr.Dataset\n        Converted SLC stack.\n        An xarray.Dataset with three dimensions: `(azimuth, range, time)`, and\n        three variables: `complex`, `amplitude`, `phase`.\n\n    Raises\n    ------\n    ValueError\n        The input dataset should have three dimensions: `(azimuth, range, time)`.\n    ValueError\n        The input dataset should have the following variables: `('real', 'imag')`.\n    \"\"\"\n    # Check ds should have the following dimensions: (azimuth, range, time)\n    if any(dim not in ds.sizes for dim in [\"azimuth\", \"range\", \"time\"]):\n        raise ValueError(\n            \"The input dataset should have three dimensions: (azimuth, range, time).\"\n        )\n\n    # Check ds should have the following variables: (\"real\", \"imag\")\n    if any(var not in ds.variables for var in [\"real\", \"imag\"]):\n        raise ValueError(\n            \"The input dataset should have the following variables: ('real', 'imag').\"\n        )\n\n    # Construct the three datavariables: complex, amplitude, and phase\n    ds[\"complex\"] = ds[\"real\"] + 1j * ds[\"imag\"]\n    ds = ds.slcstack._get_amplitude()\n    ds = ds.slcstack._get_phase()\n\n    # Remove the original real and imag variables\n    ds = ds.drop_vars([\"real\", \"imag\"])\n\n    return ds\n</code></pre>"},{"location":"api_reference/#sarxarray._io.from_binary","title":"sarxarray._io.from_binary","text":"<pre><code>from_binary(slc_files, shape, vlabel='complex', dtype=complex64, chunks=None, ratio=1)\n</code></pre> <p>Read a SLC stack or related variables from binary files.</p> <p>Parameters:</p> <ul> <li> <code>slc_files</code>               (<code>Iterable</code>)           \u2013            <p>Paths to the SLC files.</p> </li> <li> <code>shape</code>               (<code>Tuple</code>)           \u2013            <p>Shape of each SLC file, in (n_azimuth, n_range)</p> </li> <li> <code>vlabel</code>               (<code>str</code>, default:                   <code>'complex'</code> )           \u2013            <p>Name of the variable to read, by default \"complex\".</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>complex64</code> )           \u2013            <p>Data type of the file to read, by default np.float32</p> </li> <li> <code>chunks</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>2-D chunk size, by default None</p> </li> <li> <code>ratio</code>           \u2013            <p>Ratio of resolutions (azimuth/range), by default 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>An xarray.Dataset with three dimensions: (azimuth, range, time).</p> </li> </ul> Source code in <code>sarxarray/_io.py</code> <pre><code>def from_binary(\n    slc_files, shape, vlabel=\"complex\", dtype=np.complex64, chunks=None, ratio=1\n):\n    \"\"\"Read a SLC stack or related variables from binary files.\n\n    Parameters\n    ----------\n    slc_files : Iterable\n        Paths to the SLC files.\n    shape : Tuple\n        Shape of each SLC file, in (n_azimuth, n_range)\n    vlabel : str, optional\n        Name of the variable to read, by default \"complex\".\n    dtype : numpy.dtype, optional\n        Data type of the file to read, by default np.float32\n    chunks : list, optional\n        2-D chunk size, by default None\n    ratio:\n        Ratio of resolutions (azimuth/range), by default 1\n\n    Returns\n    -------\n    xarray.Dataset\n        An xarray.Dataset with three dimensions: (azimuth, range, time).\n\n    \"\"\"\n    # Check dtype\n    if not np.dtype(dtype).isbuiltin:\n        if not all([name in ((\"re\", \"im\")) for name in dtype.names]):\n            raise TypeError(\n                \"The customed dtype should have only two field names: \"\n                '\"re\" and \"im\". For example: '\n                'dtype = np.dtype([(\"re\", np.float32), (\"im\", np.float32)]).'\n            )\n\n    # Initialize stack as a Dataset\n    coords = {\n        \"azimuth\": range(shape[0]),\n        \"range\": range(shape[1]),\n        \"time\": range(len(slc_files)),\n    }\n    ds_stack = xr.Dataset(coords=coords)\n\n    # Calculate appropriate chunk size if not user-defined\n    if chunks is None:\n        chunks = _calc_chunksize(shape, dtype, ratio)\n\n    # Read in all SLCs\n    slcs = None\n    for f_slc in slc_files:\n        if slcs is None:\n            slcs = _mmap_dask_array(f_slc, shape, dtype, chunks).reshape(\n                (shape[0], shape[1], 1)\n            )\n        else:\n            slc = _mmap_dask_array(f_slc, shape, dtype, chunks).reshape(\n                (shape[0], shape[1], 1)\n            )\n            slcs = da.concatenate([slcs, slc], axis=2)\n\n    # unpack the customized dtype\n    if not np.dtype(dtype).isbuiltin:\n        meta_arr = np.array((), dtype=_dtypes[\"complex\"])\n        slcs = da.apply_gufunc(_unpack_complex, \"()-&gt;()\", slcs, meta=meta_arr)\n\n    ds_stack = ds_stack.assign({vlabel: ((\"azimuth\", \"range\", \"time\"), slcs)})\n\n    # If reading complex data, automatically\n    if vlabel == \"complex\":\n        ds_stack = ds_stack.slcstack._get_amplitude()\n        ds_stack = ds_stack.slcstack._get_phase()\n\n    return ds_stack\n</code></pre>"},{"location":"api_reference/#utility","title":"Utility","text":""},{"location":"api_reference/#sarxarray.utils.multi_look","title":"sarxarray.utils.multi_look","text":"<pre><code>multi_look(data, window_size, method='coarsen', statistics='mean', compute=True)\n</code></pre> <p>Perform multi-looking on a Stack, and return a Stack.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dataset or DataArray</code>)           \u2013            <p>The data to be multi-looked.</p> </li> <li> <code>window_size</code>               (<code>tuple</code>)           \u2013            <p>Window size for multi-looking, in the format of (azimuth, range)</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'coarsen'</code> )           \u2013            <p>Method of multi-looking, by default \"coarsen\"</p> </li> <li> <code>statistics</code>               (<code>str</code>, default:                   <code>'mean'</code> )           \u2013            <p>Statistics method for multi-looking, by default \"mean\"</p> </li> <li> <code>compute</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the result, by default True. If False, the result will be <code>dask.delayed.Delayed</code>. This is useful when the multi_look is used as an intermediate result.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset or DataArray</code>           \u2013            <p>An <code>xarray.Dataset</code> or <code>xarray.DataArray</code> with coarsen shape if <code>compute</code> is True, otherwise a <code>dask.delayed.Delayed</code> object.</p> </li> </ul> Source code in <code>sarxarray/utils.py</code> <pre><code>def multi_look(data, window_size, method=\"coarsen\", statistics=\"mean\", compute=True):\n    \"\"\"Perform multi-looking on a Stack, and return a Stack.\n\n    Parameters\n    ----------\n    data : xarray.Dataset or xarray.DataArray\n        The data to be multi-looked.\n    window_size : tuple\n        Window size for multi-looking, in the format of (azimuth, range)\n    method : str, optional\n        Method of multi-looking, by default \"coarsen\"\n    statistics : str, optional\n        Statistics method for multi-looking, by default \"mean\"\n    compute : bool, optional\n        Whether to compute the result, by default True. If False, the result\n        will be `dask.delayed.Delayed`. This is useful when the multi_look\n        is used as an intermediate result.\n\n    Returns\n    -------\n    xarray.Dataset or xarray.DataArray\n        An `xarray.Dataset` or `xarray.DataArray` with coarsen shape if\n        `compute` is True, otherwise a `dask.delayed.Delayed` object.\n    \"\"\"\n    # validate the input\n    _validate_multi_look_inputs(data, window_size, method, statistics)\n\n    # chunk data if not already chunked\n    if isinstance(data, Delayed) or not data.chunks:\n        data = data.chunk(\"auto\")\n\n    # get the chunk size\n    if isinstance(data, Delayed):\n        chunks = \"auto\"\n    else:\n        chunks = _get_chunks(data, window_size)\n\n    # add new atrrs here because Delayed objects are immutable\n    data.attrs[\"multi-look\"] = f\"{method}-{statistics}\"\n\n    # define custom coordinate function to define new coordinates starting\n    # from 0: the inputs `reshaped` and `axis` are output of\n    # `coarsen_reshape` internal function and are passed to the `coord_func`\n    def _custom_coord_func(reshaped, axis):\n        if axis[0] == 1 or axis[0] == 2:\n            return np.arange(0, reshaped.shape[0], 1, dtype=int)\n        else:\n            return reshaped.flatten()\n\n    if method == \"coarsen\":\n        # TODO: if boundary and size should be configurable\n        multi_looked = data.coarsen(\n            {\"azimuth\": window_size[0], \"range\": window_size[1]},\n            boundary=\"trim\",\n            side=\"left\",\n            coord_func=_custom_coord_func,\n        )\n\n    # apply statistics\n    stat_functions = {\n        \"mean\": multi_looked.mean,\n        \"median\": multi_looked.median,\n    }\n\n    stat_function = stat_functions[statistics]\n    if compute:\n        multi_looked = stat_function(keep_attrs=True)\n    else:\n        multi_looked = delayed(stat_function)(keep_attrs=True)\n\n    # Rechunk is needed because shape of the data will be changed after\n    # multi-looking\n    multi_looked = multi_looked.chunk(chunks)\n\n    return multi_looked\n</code></pre>"},{"location":"api_reference/#sarxarray.utils.complex_coherence","title":"sarxarray.utils.complex_coherence","text":"<pre><code>complex_coherence(reference: DataArray, other: DataArray, window_size, compute=True)\n</code></pre> <p>Calculate complex coherence of two images.</p> <p>Assume two images reference (R) and other (O), the complex coherence is defined as: numerator = mean(R * O<code>) in a window denominator = mean(R * R</code>) * mean(O * O`) in a window coherence = abs( numerator / sqrt(denominator) ), See the equation in chapter 28 in doris documentation</p> <p>Parameters:</p> <ul> <li> <code>reference</code>               (<code>DataArray</code>)           \u2013            <p>The reference image to calculate complex coherence with.</p> </li> <li> <code>other</code>               (<code>DataArray</code>)           \u2013            <p>The other image to calculate complex coherence with.</p> </li> <li> <code>window_size</code>               (<code>tuple</code>)           \u2013            <p>Window size for multi-looking, in the format of (azimuth, range)</p> </li> <li> <code>compute</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the result, by default True. If False, the result will be <code>dask.delayed.Delayed</code>. This is useful when the complex_coherence is used as an intermediate result.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataArray</code>           \u2013            <p>An <code>xarray.DataArray</code> if <code>compute</code> is True, otherwise a <code>dask.delayed.Delayed</code> object.</p> </li> </ul> Source code in <code>sarxarray/utils.py</code> <pre><code>def complex_coherence(\n    reference: xr.DataArray, other: xr.DataArray, window_size, compute=True\n):\n    \"\"\"Calculate complex coherence of two images.\n\n    Assume two images reference (R) and other (O), the complex coherence is\n    defined as:\n    numerator = mean(R * O`) in a window\n    denominator = mean(R * R`) * mean(O * O`) in a window\n    coherence = abs( numerator / sqrt(denominator) ),\n    See the equation in chapter 28 in [doris\n    documentation](http://doris.tudelft.nl/software/doris_v4.02.pdf)\n\n    Parameters\n    ----------\n    reference : xarray.DataArray\n        The reference image to calculate complex coherence with.\n    other : xarray.DataArray\n        The other image to calculate complex coherence with.\n    window_size : tuple\n        Window size for multi-looking, in the format of (azimuth, range)\n    compute : bool, optional\n        Whether to compute the result, by default True. If False, the result\n        will be `dask.delayed.Delayed`. This is useful when the complex_coherence\n        is used as an intermediate result.\n\n    Returns\n    -------\n    xarray.DataArray\n        An `xarray.DataArray` if `compute` is True,\n        otherwise a `dask.delayed.Delayed` object.\n    \"\"\"\n    # check if the two images have the same shape\n    if (\n        reference.azimuth.size != other.azimuth.size\n        or reference.range.size != other.range.size\n    ):\n        raise ValueError(\"The two images have different shape.\")\n\n    # check if dtype is complex\n    if reference.dtype != np.complex64 or other.dtype != np.complex64:\n        raise ValueError(\"The dtype of the two images must be complex64.\")\n\n    # calculate the numerator of the equation\n    da = reference * other.conj()\n    numerator = multi_look(\n        da, window_size, method=\"coarsen\", statistics=\"mean\", compute=compute\n    )\n\n    # calculate the denominator of the equation\n    da = reference * reference.conj()\n    reference_mean = multi_look(\n        da, window_size, method=\"coarsen\", statistics=\"mean\", compute=compute\n    )\n\n    da = other * other.conj()\n    other_mean = multi_look(\n        da, window_size, method=\"coarsen\", statistics=\"mean\", compute=compute\n    )\n\n    denominator = reference_mean * other_mean\n\n    # calculate the coherence\n    def _compute_coherence(numerator, denominator):\n        return np.abs(numerator / np.sqrt(denominator))\n\n    if compute:\n        coherence = _compute_coherence(numerator, denominator)\n    else:\n        coherence = delayed(_compute_coherence)(numerator, denominator)\n\n    return coherence\n</code></pre>"},{"location":"common_ops/","title":"Common SLC operations","text":"<p>Details about the common operations in this page are coming soon...</p> <p>Common SAR processings can be performed by SARXarray. Below are some examples:</p>"},{"location":"common_ops/#multi-look","title":"Multi-look","text":"<p>Multi-look by a windowsize of e.g. 2 in azimuth dimension and 4 in range dimension:</p> <pre><code>stack_multilook = stack.slcstack.multi_look((2,4))\n</code></pre>"},{"location":"common_ops/#coherence","title":"Coherence","text":"<p>Compute coherence between two SLCs:</p> <pre><code>from sarxarray import complex_coherence\nslc1 = stack.complex.isel(time=0) # first image\nslc2 = stack.complex.isel(time=2) # third image\nwindow = (4,4)\n\ncoherence = complex_coherence(slc1, slc2, window)\n</code></pre>"},{"location":"common_ops/#mean-reflection-map-mrm","title":"Mean-Reflection-Map (MRM)","text":"<pre><code>mrm = stack_multilook.slcstack.mrm()\n</code></pre> <pre><code>from matplotlib import pyplot as plt\nfig, ax = plt.subplots()\nax.imshow(mrm)\nmrm.plot(ax=ax, robust=True, cmap='gray')\n</code></pre>"},{"location":"common_ops/#point-selection","title":"Point selection","text":"<p>A selection based on temporal properties per pixel can be performed. For example, we can select the Point Scatterers (PS) by normalized temporal dispersion of <code>amplitude</code>:</p> <pre><code>ps = stack.slcstack.point_selection(threshold=0.25, method=\"amplitude_dispersion\")\n</code></pre>"},{"location":"data_loading/","title":"Usage","text":""},{"location":"data_loading/#input-data-format","title":"Input data format","text":"<p>SARXarray works with coregistered SLC/interferogram stack. SARXarray provides a reader to perform lazy loading on data stacks in different file formats, including binary format. However, we recommend to store the coregistered stack in <code>zarr</code> format, and directly load them as an Xarray object by <code>xarray.open_zarr</code>. </p>"},{"location":"data_loading/#loading-coregistered-slc-stack-in-binary-format","title":"Loading coregistered SLC stack in binary format","text":"<p>If the stack is saved in binary format, it can be read by <code>SARXarray</code> under two pre-requisites:</p> <ol> <li>All SLCs/interferograms have the same known raster size and data type;</li> <li>All SLCs/interferograms have been resampled to the same raster grid.</li> </ol> <p>For example, let's consider a case of a stack with three SLCs:</p> <pre><code>import numpy as np\nlist_slcs = ['data/slc_1.raw', 'data/slc_2.raw', 'data/slc_3.raw']\nshape = (10018, 68656) # (azimuth, range)\ndtype = np.complex64\n</code></pre> <p>We built a list <code>list_slcs</code> with the paths to the SLCs. In this case they are stored in the same directory called <code>data</code>. The shape of each SLC should be provided, i.e.: <code>10018</code> pixels in <code>azimuth</code> direction, and <code>68656</code> in range direction. The data type is <code>numpy.complex64</code>.</p> <p>The coregistered SLC stack can be read using the <code>from_binary</code> function:</p> <p><pre><code>import sarxarray\n\nstack = sarxarray.from_binary(list_slcs, shape, dtype=dtype)\n</code></pre> You can also skip the <code>dtype</code> argument since it's defaulted to <code>numpy.complex64</code>. The stack will be read as an <code>xarray.Dataset</code> object, with data variables lazily loaded as <code>Dask Array</code>:</p> <pre><code>print(stack)\n</code></pre> <pre><code>&lt;xarray.Dataset&gt;\nDimensions:    (azimuth: 10018, range: 68656, time: 3)\nCoordinates:\n  * azimuth    (azimuth) int64 0 1 2 3 4 5 ... 10013 10014 10015 10016 10017\n  * range      (range) int64 0 1 2 3 4 5 ... 68650 68651 68652 68653 68654 68655\n  * time       (time) int64 0 1 2\nData variables:\n    complex    (azimuth, range, time) complex64 dask.array&lt;chunksize=(4000, 4000, 1), meta=np.ndarray&gt;\n    amplitude  (azimuth, range, time) float32 dask.array&lt;chunksize=(4000, 4000, 1), meta=np.ndarray&gt;\n    phase      (azimuth, range, time) float32 dask.array&lt;chunksize=(4000, 4000, 1), meta=np.ndarray&gt;\n</code></pre> <p>The loading chunk size can also be specified manually:</p> <pre><code>stack_smallchunk = sarxarray.from_binary(list_slcs, shape, chunks=(2000, 2000))\n</code></pre>"},{"location":"data_loading/#reading-metadata","title":"Reading metadata","text":"<p>SARXarray provides a function to read metadata from the interferogram stack coregistered by Doris v4 or Doris v5. The metadata is read as a dictionary from the <code>slave.res</code> file under the folder of each SLC.</p>"},{"location":"data_loading/#doris-v4-metadata","title":"Doris v4 metadata","text":"<p>A common Doris v4 output folder structure is as follows:</p> <pre><code>stack/\n\u251c\u2500\u2500 YYYYMMDD1/\n\u2502   \u251c\u2500\u2500 slc_1.res\n\u2502   \u251c\u2500\u2500 slc_1.raw\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 YYYYMMDD2/\n\u2502   \u251c\u2500\u2500 slc_2.res\n\u2502   \u251c\u2500\u2500 slc_2.raw\n\u2502   \u251c\u2500\u2500 ...\n...\n</code></pre> <p>Where <code>YYYYMMDD1</code>, <code>YYYYMMDD2</code>, etc. are the acquisition dates of the SLCs, and <code>slc_1.res</code>, <code>slc_2.res</code>, etc. are the metadata files for each SLC.</p> <p>To read the metadata from the Doris v4 stack, first build a list of the SLC metadata files:</p> <pre><code>from pathlib import Path\nstack_folder = Path('stack/')\nres_file_list = list(tsx_folder.glob('???????/slave.res'))\n</code></pre> <p>where the pattern <code>???????</code> matches the date folders. Then, you can use the <code>read_metadata</code> function with the <code>driver</code> argument set to <code>\"doris4\"</code>:</p> <pre><code>import sarxarray\nmetadata = sarxarray.read_metadata(res_file_list, driver=\"doris4\")\n</code></pre>"},{"location":"data_loading/#doris-v5-metadata","title":"Doris v5 metadata","text":"<p>A common Doris v5 output folder structure is as follows:</p> <pre><code>\u251c\u2500\u2500 YYYYMMDD1/\n\u2502   \u251c\u2500\u2500 slc_1.res\n\u2502   \u251c\u2500\u2500 ifgs_1.res\n\u2502   \u251c\u2500\u2500 slc_1.raw\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 YYYYMMDD2/\n\u2502   \u251c\u2500\u2500 slc_2.res\n\u2502   \u251c\u2500\u2500 ifgs_2.res\n\u2502   \u251c\u2500\u2500 slc_2.raw\n\u2502   \u251c\u2500\u2500 ...\n...\n</code></pre> <p>Where <code>YYYYMMDD1</code>, <code>YYYYMMDD2</code>, etc. are the acquisition dates of the SLCs, and <code>slc_1.res</code>, <code>slc_2.res</code>, etc. are the metadata files for each SLC. The files <code>ifgs_1.res</code>, <code>ifgs_2.res</code>, etc. are the metadata files for each interferogram, which contain the information of the sizes of the coregistered interferograms.</p> <p>To read the metadata from the Doris v5 stack, first build a list of the SLC metadata files:</p> <pre><code>from pathlib import Path\nstack_folder = Path('stack/')\nres_file_list = list(stack_folder.glob('???????/slc_*.res'))\n</code></pre> <p>Then, you can use the <code>read_metadata</code> function with the <code>driver</code> argument set to <code>\"doris5\"</code>:</p> <pre><code>import sarxarray\nmetadata = sarxarray.read_metadata(res_file_list, driver=\"doris5\")\n</code></pre> <p><code>read_metadata</code> assumes that <code>ifgs_*.res</code> files are in the same folder as the <code>slc_*.res</code> files, and will read the interferogram sizes from them.</p>"},{"location":"manipulations/","title":"Manipulate an SLC stack as an Xarray","text":"<p>The loaded stack can be manipulated as an <code>Xarray.Dataset</code> instance.</p> <p>Slice the SLC stack in 3D:</p> <pre><code>stack.isel(azimuth=range(1000,2000), range=range(1500,2500), time=range(2,5))\n</code></pre> <p>Select the <code>amplitude</code> attribute <pre><code>amp = stack['amplitude']\n</code></pre></p> <p>Compute stack and persist in memory: <pre><code>stack = stack.compute()\n</code></pre></p>"},{"location":"setup/","title":"Installation","text":"<p>SARXarray can be installed from PyPI:</p> <pre><code>pip install sarxarray\n</code></pre> <p>or from the source:</p> <pre><code>git clone git@github.com:TUDelftGeodesy/sarxarray.git\ncd sarxarray\npip install .\n</code></pre> <p>Note that Python version <code>&gt;=3.10</code> is required for SARXarray.</p>"},{"location":"setup/#tips","title":"Tips","text":"<p>We strongly recommend installing separately from your default Python environment. E.g. you can use environment manager (e.g. mamba) to create a separate environment.</p>"},{"location":"notebooks/demo_sarxarray/","title":"Example Notebook","text":"<p>You can download this Jupyter Notebook via the download button at the top of this page.</p> <p>In this Jupyter Notebook, we demonstrate the following operations using <code>sarxarray</code>:</p> <ul> <li>Load an interferogram stack in binary format into a <code>xarray.Dataset</code> object;</li> <li>Append latitude and longitude coordinates to the loaded stack;</li> <li>Create an Mean-Reflection-Map (MRM) of a subset of the interferogram stack;</li> <li>Apply common SAR processing steps to the interferogram stack;</li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom pathlib import Path\nimport sarxarray\n</pre> import numpy as np from pathlib import Path import sarxarray <p>We will load the interferogram stack, which has been coregistered and saved as binary files. We assume the shape and data type is known.</p> In\u00a0[2]: Copied! <pre># Path to the interferogram dataset\npath = Path('nl_amsterdam_s1_asc_t088')\n\n# Make a list of interferograms to read\nlist_ifgs = [p for p in path.rglob('*_cint_srd.raw')]\nlist_ifgs.sort()\n\n# Metadata of the stack, assume known.\nshape=(2000, 4000)\n\n# Define reading chunks\nreading_chunks = (500,500)\n</pre> # Path to the interferogram dataset path = Path('nl_amsterdam_s1_asc_t088')  # Make a list of interferograms to read list_ifgs = [p for p in path.rglob('*_cint_srd.raw')] list_ifgs.sort()  # Metadata of the stack, assume known. shape=(2000, 4000)  # Define reading chunks reading_chunks = (500,500) In\u00a0[3]: Copied! <pre># Check the list of interferograms\nprint(list_ifgs)\n</pre> # Check the list of interferograms print(list_ifgs) <pre>[PosixPath('nl_amsterdam_s1_asc_t088/20180920_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20180926_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181002_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181008_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181014_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181020_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181026_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181101_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181107_cint_srd.raw'), PosixPath('nl_amsterdam_s1_asc_t088/20181113_cint_srd.raw')]\n</pre> <p>Use <code>from_binary</code> to load the stack:</p> In\u00a0[4]: Copied! <pre># Load complex data\nstack = sarxarray.from_binary(list_ifgs, shape, dtype=np.complex64, chunks=reading_chunks)\n\nprint(stack)\n</pre> # Load complex data stack = sarxarray.from_binary(list_ifgs, shape, dtype=np.complex64, chunks=reading_chunks)  print(stack) <pre>&lt;xarray.Dataset&gt;\nDimensions:    (azimuth: 2000, range: 4000, time: 10)\nCoordinates:\n  * azimuth    (azimuth) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * range      (range) int64 0 1 2 3 4 5 6 ... 3994 3995 3996 3997 3998 3999\n  * time       (time) int64 0 1 2 3 4 5 6 7 8 9\nData variables:\n    complex    (azimuth, range, time) complex64 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n    amplitude  (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n    phase      (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre># Geo-coordinates\nf_lat = [path / 'lat.raw']\nf_lon = [path / 'lon.raw']\n</pre> # Geo-coordinates f_lat = [path / 'lat.raw'] f_lon = [path / 'lon.raw'] In\u00a0[6]: Copied! <pre># Load coordinates\nlat = sarxarray.from_binary(\n    f_lat, shape, vlabel=\"lat\", dtype=np.float32, chunks=reading_chunks\n)\nlon = sarxarray.from_binary(\n    f_lon, shape, vlabel=\"lon\", dtype=np.float32, chunks=reading_chunks\n)\nstack = stack.assign_coords(\n    lat=((\"azimuth\", \"range\"), lat.squeeze().lat.data),\n    lon=((\"azimuth\", \"range\"), lon.squeeze().lon.data),\n)\n</pre> # Load coordinates lat = sarxarray.from_binary(     f_lat, shape, vlabel=\"lat\", dtype=np.float32, chunks=reading_chunks ) lon = sarxarray.from_binary(     f_lon, shape, vlabel=\"lon\", dtype=np.float32, chunks=reading_chunks ) stack = stack.assign_coords(     lat=((\"azimuth\", \"range\"), lat.squeeze().lat.data),     lon=((\"azimuth\", \"range\"), lon.squeeze().lon.data), ) In\u00a0[7]: Copied! <pre>print(stack)\n</pre> print(stack) <pre>&lt;xarray.Dataset&gt;\nDimensions:    (azimuth: 2000, range: 4000, time: 10)\nCoordinates:\n  * azimuth    (azimuth) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * range      (range) int64 0 1 2 3 4 5 6 ... 3994 3995 3996 3997 3998 3999\n  * time       (time) int64 0 1 2 3 4 5 6 7 8 9\n    lat        (azimuth, range) float32 dask.array&lt;chunksize=(500, 500), meta=np.ndarray&gt;\n    lon        (azimuth, range) float32 dask.array&lt;chunksize=(500, 500), meta=np.ndarray&gt;\nData variables:\n    complex    (azimuth, range, time) complex64 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n    amplitude  (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n    phase      (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 500, 1), meta=np.ndarray&gt;\n</pre> <p>Some common SAR operations are supported by SARXarray.</p> In\u00a0[8]: Copied! <pre>stack_multilook = stack.slcstack.multi_look((1,2))\nprint(stack_multilook)\n</pre> stack_multilook = stack.slcstack.multi_look((1,2)) print(stack_multilook) <pre>&lt;xarray.Dataset&gt;\nDimensions:    (azimuth: 2000, range: 2000, time: 10)\nCoordinates:\n  * azimuth    (azimuth) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * range      (range) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * time       (time) int64 0 1 2 3 4 5 6 7 8 9\n    lat        (azimuth, range) float32 dask.array&lt;chunksize=(500, 250), meta=np.ndarray&gt;\n    lon        (azimuth, range) float32 dask.array&lt;chunksize=(500, 250), meta=np.ndarray&gt;\nData variables:\n    complex    (azimuth, range, time) complex64 dask.array&lt;chunksize=(500, 250, 1), meta=np.ndarray&gt;\n    amplitude  (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 250, 1), meta=np.ndarray&gt;\n    phase      (azimuth, range, time) float32 dask.array&lt;chunksize=(500, 250, 1), meta=np.ndarray&gt;\nAttributes:\n    multi-look:  coarsen-mean\n</pre> In\u00a0[9]: Copied! <pre>mrm = stack_multilook.slcstack.mrm()\nprint(mrm)\n</pre> mrm = stack_multilook.slcstack.mrm() print(mrm) <pre>&lt;xarray.DataArray 'amplitude' (azimuth: 2000, range: 2000)&gt;\ndask.array&lt;mean_agg-aggregate, shape=(2000, 2000), dtype=float32, chunksize=(500, 250), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * azimuth  (azimuth) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * range    (range) int64 0 1 2 3 4 5 6 ... 1993 1994 1995 1996 1997 1998 1999\n    lat      (azimuth, range) float32 dask.array&lt;chunksize=(500, 250), meta=np.ndarray&gt;\n    lon      (azimuth, range) float32 dask.array&lt;chunksize=(500, 250), meta=np.ndarray&gt;\n</pre> In\u00a0[10]: Copied! <pre># Visualize\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots()\nfig.set_size_inches((5,5))\nax.imshow(mrm)\nim = mrm.plot(ax=ax, robust=True, cmap='gray')\nim.set_clim([0, 50000])\n</pre> # Visualize from matplotlib import pyplot as plt fig, ax = plt.subplots() fig.set_size_inches((5,5)) ax.imshow(mrm) im = mrm.plot(ax=ax, robust=True, cmap='gray') im.set_clim([0, 50000])"},{"location":"notebooks/demo_sarxarray/#example-notebook","title":"Example Notebook\u00b6","text":""},{"location":"notebooks/demo_sarxarray/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/demo_sarxarray/#data-preparation","title":"Data preparation\u00b6","text":"<p>We use a coregistered and georeferenced Sentinel-1 interferogram stack over Amsterdam as an example dataset. Please download the data and unzip it locally.</p>"},{"location":"notebooks/demo_sarxarray/#environment-setup","title":"Environment setup\u00b6","text":"<p>For the python environment setup, we assume you already installed SARXarray following the installation guide.</p> <p>Some extra python dependencies are required to execute this notebook. You can install the extra python dependencies by:</p> <pre>pip install sarxarray[demo]\n</pre> <p>After installation, execute the notebook in a JupyterLab session, which can be started by running the <code>jupyter-lab</code> command in your command line:</p> <pre>jupyter-lab\n</pre> <p>Alternatively, you can also use Jupyter Notebook.</p> <p>A new tab will be opened in your default browser to execute this notebook.</p>"},{"location":"notebooks/demo_sarxarray/#data-loading","title":"Data loading\u00b6","text":""},{"location":"notebooks/demo_sarxarray/#append-georeferenced-coordinates","title":"Append georeferenced coordinates\u00b6","text":"<p>The <code>sarxarray</code> is implemented as an extension of <code>Xarray.Dataset</code>, which means we can modify the <code>stack</code> variable as a normal <code>Dataset</code> object. For example, we can append geo-coordinates to the loaded stack.</p>"},{"location":"notebooks/demo_sarxarray/#common-sar-operations","title":"Common SAR operations\u00b6","text":""},{"location":"notebooks/demo_sarxarray/#multi-looking","title":"Multi-Looking\u00b6","text":"<p>We apply a <code>(1,2)</code> multi-look to the loaded stack. As a result the size of the output will be <code>(2000,2000)</code>.</p>"},{"location":"notebooks/demo_sarxarray/#mean-reflection-map-mrm","title":"Mean-Reflection-Map (MRM)\u00b6","text":""}]}